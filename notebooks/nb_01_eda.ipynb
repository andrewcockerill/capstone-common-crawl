{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis of Common Crawl Web Archive Data\n",
    "\n",
    "In this notebook, we perform an initial assessment of the nature of the data provided in the web archive (WARC) files made available in the <a href=\"https://commoncrawl.org/\">Common Crawl</a> web data repository. This resource consists of several hundred terabytes of web crawl data including related HTML content and headers, which we will explore here. We will make note of the different data elements in these WARC files, how they can be interpreted, and how we might transform these into a tabular form for downstream storage and analysis.\n",
    "\n",
    "For the purposes of this EDA, we will narrow the scope to just a single WARC file to begin characterizing the data. Here, the <a href=\"https://resiliparse.chatnoir.eu/en/stable/man/fastwarc.html\">FastWARC</a> package is used for the purposes of opening and exploring each web archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from fastwarc.warc import *\n",
    "from fastwarc.stream_io import GZipStream\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pprint\n",
    "from timeit import default_timer as timer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on WARC Record Types\n",
    "Via the documentation on the FastWARC package, we note that each record in a WARC is not necessarily an HTML response containing HTML headers/code but can also simply consist of only of a request record. As analytics use cases would more likely be interested in records with actual HTML data, we can limit our results to these types of response records. By referencing the documentation, we can see how to create an iterator object that will serve as a generator for response records only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_LOC = \"../data\"\n",
    "WARCS = [os.path.join(DATA_LOC, i) for i in os.listdir(DATA_LOC) if bool(re.search(\"\\.warc.gz$\",i))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def parse_record(record, return_html=False):\n",
    "    \"\"\"Extract WARC/HTTP header and optionally the full HTML\"\"\"\n",
    "\n",
    "    # Headers\n",
    "    warc_header = record.headers\n",
    "    http_header = record.http_headers\n",
    "\n",
    "    # Raw HTML\n",
    "    raw_html = record.reader.read()\n",
    "        \n",
    "    # Output\n",
    "    if return_html:\n",
    "        return (warc_header, http_header, raw_html)\n",
    "    else:\n",
    "        return (warc_header, http_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Record\n",
    "\n",
    "We can begin by extracting the WARC headers, and payload from just a single record known to have all of these elements available. As mentioned, the FastWARC package allows for custom filters, so in this case we filter for a specific site's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull an example record\n",
    "\n",
    "with (open(WARCS[0], 'rb')) as stream:\n",
    "    example_uri = 'http://1836saloon.net/2019/09/22/specials-for-the-week-of-9-22-thru-9-28/'\n",
    "    example_iterator = ArchiveIterator(stream, func_filter=\n",
    "                                    lambda x: (x.headers.get('WARC-Type')=='response') and (x.headers.get('WARC-Target-URI')==example_uri))\n",
    "    \n",
    "    example_record = next(example_iterator)\n",
    "\n",
    "    warc_header, http_header, raw_html = parse_record(example_record, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WARC Headers\n",
    "\n",
    "From the site documentation, we can see that the web archive files used by Common Crawl use set headers that convey different types of information. These named fields follow a standardized format, and we can leverage a resource <a href=\"https://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.1/#named-fields\">offered by Github</a> which contains additional information about these fields. One item of analytics interest is the content length, which stores the size of the response in bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WARC-Type': 'response',\n",
      " 'WARC-Date': '2023-02-06T06:46:56Z',\n",
      " 'WARC-Record-ID': '<urn:uuid:f863b6b3-0632-4524-9b1a-e7cd3ea7a231>',\n",
      " 'Content-Length': '43014',\n",
      " 'Content-Type': 'application/http; msgtype=response',\n",
      " 'WARC-Warcinfo-ID': '<urn:uuid:2c7f7c4e-d996-4046-a11d-ce5ea7367928>',\n",
      " 'WARC-Concurrent-To': '<urn:uuid:82c7afee-8991-4186-a2ec-a54984ced648>',\n",
      " 'WARC-IP-Address': '50.63.8.18',\n",
      " 'WARC-Target-URI': 'http://1836saloon.net/2019/09/22/specials-for-the-week-of-9-22-thru-9-28/',\n",
      " 'WARC-Payload-Digest': 'sha1:B5R5RCDZQDU5DJSEMCNXL7M3ENJCRO5Y',\n",
      " 'WARC-Block-Digest': 'sha1:EIXW4QUYQBU6D3RLK4VHEFXVJ5U7U3LH',\n",
      " 'WARC-Identified-Payload-Type': 'text/html'}\n"
     ]
    }
   ],
   "source": [
    "# View the WARC Header\n",
    "pprint.pp(dict(warc_header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTTP Headers\n",
    "We can simlarly view the output of the HTTP headers. As described in <a href=\"https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\">this article</a>, many types of HTTP headers exist and will by no means be uniform across different websites. While many fields exist, the following data elements could be of interest to analytics use cases:\n",
    "\n",
    "- Server: Name of the server hosting the site\n",
    "\n",
    "- Content-Language: Language(s) for the intended audience of the site\n",
    "\n",
    "- Referer: This denotes the website from which a link was used to access the page\n",
    "\n",
    "- Last-Modified: Date in which the object was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Mon, 06 Feb 2023 06:46:51 GMT',\n",
      " 'Server': 'Apache',\n",
      " 'X-Pingback': 'http://1836saloon.net/xmlrpc.php',\n",
      " 'Link': '<http://1836saloon.net/wp-json/>; rel=\"https://api.w.org/\", '\n",
      "         '<http://1836saloon.net/?p=529>; rel=shortlink',\n",
      " 'Upgrade': 'h2,h2c',\n",
      " 'Connection': 'Upgrade, Keep-Alive',\n",
      " 'Vary': 'Accept-Encoding',\n",
      " 'X-Crawler-Content-Encoding': 'br',\n",
      " 'X-Crawler-Content-Length': '10028',\n",
      " 'Content-Length': '42559',\n",
      " 'Keep-Alive': 'timeout=5',\n",
      " 'Content-Type': 'text/html; charset=UTF-8'}\n"
     ]
    }
   ],
   "source": [
    "# View the HTTP Headers\n",
    "pprint.pp(dict(http_header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raw HTML\n",
    "The raw HTML code is also available for view. At this point, we can simply consider ingesting the code, but we may wish to parse these records at some point to extract elements of interest like page titles. In this case, we can see that the content of this page pertained to a special being offered at a restaurant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'html': '\\r\\n'\n",
      "         '<!DOCTYPE html>\\n'\n",
      "         '<html lang=\"en-US\">\\n'\n",
      "         '<head>\\n'\n",
      "         '<meta charset=\"UTF-8\">\\n'\n",
      "         '<meta name=\"viewport\" content=\"width=device-width, '\n",
      "         'initial-scale=1\">\\n'\n",
      "         '<link rel=\"profile\" href=\"http://gmpg.org/xfn/11\">\\n'\n",
      "         '<link rel=\"pingback\" href=\"http://1836saloon.net/xmlrpc.php\">\\n'\n",
      "         '\\n'\n",
      "         '<!-- WP_Head -->\\n'\n",
      "         '<title>SPECIALS FOR THE WEEK OF 9/22 THRU 9/28 &#8211; 1836 Saloon '\n",
      "         '&amp; Grille</title>\\n'\n",
      "         '<meta property=\"og:title\" content=\"SPECIALS FOR THE WEEK OF 9/22 '\n",
      "         'THRU 9/28\"/>\\n'\n",
      "         '<meta property=\"og:description\" content=\"SUNDAY FOOTBALL SPECIALS '\n",
      "         'CHANGE WEEKLY SO COME ON IN TO SEE WHATS NEW!!!!!!!!!!!!!!  DAILY '\n",
      "         'APPETIZERS:  $3.75 CUP OF THE SOUP OF THE DAY  $3.95 CORN NUGG\"/>\\n'\n",
      "         '<meta property=\"og:url\" '\n",
      "         'content=\"http://1836saloon.net/2019/09/22/specials-for-the-week-of-9-22-thru-9-28/\"/>\\n'\n",
      "         '<meta name=\"twitter:card\" content=\"summary\">\\n'\n",
      "         '<meta property=\"twitter:title\" content=\"SPECIALS FOR THE WEEK OF '\n",
      "         '9/22 THRU 9/28\"/>\\n'\n",
      "         '<meta property=\"twitter:description\" content=\"SUNDAY FOOTBALL '\n",
      "         'SPECIALS CHANGE WEEKLY SO COME ON IN TO SEE WHATS NEW!!!!!!!!!!!!!! '}\n"
     ]
    }
   ],
   "source": [
    "pprint.pp({'html':raw_html.decode('utf-8')[0:1000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for Extraction at Scale\n",
    "There are a few things we may wish to consider regarding how we may ingest these at scale. To review, a single crawl usually consists of ~90,000 WARC files. While we will not be ingesting the entire Common Crawl dataset, we would like to explore how long processing takes for a given file.\n",
    "\n",
    "To start, we can view how many total records are in a single WARC file before seeing what the effects are of running extractions on the limited the result set. We can see that even if we were to ingest just one crawl and assuming similar read times per file, it could potentially take a long time to ingest a large number of files. We may wish to explore options available on distributed platforms to reduce ingestion times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warc_simple_iteration(path):\n",
    "    num_records = 0\n",
    "    with GZipStream(open(WARCS[0], 'rb')) as stream:\n",
    "        iterator = ArchiveIterator(stream)\n",
    "        for record in iterator:\n",
    "            num_records += 1\n",
    "    return num_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated through 108754 total records in 9.113 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_records = warc_simple_iteration(WARCS[0])\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "\n",
    "print(f\"Iterated through {num_records} total records in {elapsed_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warc_to_dictionary(path):\n",
    "    \"\"\"Create a dictionary of WARC records with HTTP responses\"\"\"\n",
    "    num_records = 0\n",
    "    with GZipStream(open(WARCS[0], 'rb')) as stream:\n",
    "        iterator = ArchiveIterator(stream, func_filter=lambda x: x.headers.get('WARC-Type')=='response')\n",
    "        warc_headers = []\n",
    "        http_headers = []\n",
    "        raw_html = []\n",
    "        for record in iterator:\n",
    "            warc_headers.append(record.headers)\n",
    "            http_headers.append(record.http_headers)\n",
    "            raw_html.append(record.reader.read())\n",
    "            num_records += 1\n",
    "    \n",
    "    proposed_output = {'num_records': num_records,\n",
    "              'warc_headers': [dict(i) for i in warc_headers],\n",
    "              'http_headers': [dict(i) for i in http_headers],\n",
    "              'raw_html': raw_html\n",
    "              }\n",
    "    \n",
    "    return proposed_output['num_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained usable results for 36251 records in 15.709 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_records = warc_to_dictionary(WARCS[0])\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "\n",
    "print(f\"Obtained usable results for {num_records} records in {elapsed_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction and Data Exploration in Spark\n",
    "One option is to leverage Spark for the purposes of ingestion using distributed computing. In this case, we explore a Pyspark implementation documented in the <a href=\"https://github.com/edsu/spn#readme\">Save Page Now</a> Github repository which explores the use of Spark with WARC data ingestion. Performance is difficult to gauge at this point due to running Spark just on a local machine, but this methodology can be used to continue our exploration and start to formulate how we will ingest this data at scale.\n",
    "\n",
    "Here, we will build a simple schema consisting of different header elements to finalize the EDA. We could also use this as a starting point for building the actual pipeline later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark packages and session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "conf = SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup schema\n",
    "field_dict = {\n",
    "    'headers':\n",
    "        {\n",
    "            'WARC-Record-ID': 'warc_record_id',\n",
    "            'WARC-Date': 'warc_date',\n",
    "            'Content-Length': 'content_length',\n",
    "            'Content-Type': 'content_type',\n",
    "            'WARC-Target-URI': 'warc_target_uri'\n",
    "        },\n",
    "    'http_headers':\n",
    "        {\n",
    "            'Server': 'server_name',\n",
    "            'Content-Language': 'content_language',\n",
    "            'Referer': 'referer',\n",
    "            'Last-Modified': 'last_modified'\n",
    "        }\n",
    "\n",
    " }\n",
    "\n",
    "sorted_header_keys = sorted(field_dict['headers'].keys())\n",
    "sorted_http_header_keys = sorted(field_dict['http_headers'].keys())\n",
    "sorted_keys = sorted_header_keys + sorted_http_header_keys\n",
    "schema = T.StructType([T.StructField(field_dict['headers'][i], T.StringType(), True) for i in sorted_header_keys] +\n",
    "                      [T.StructField(field_dict['http_headers'][i], T.StringType(), True) for i in sorted_http_header_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper unction to extract WARC files via Pyspark\n",
    "def extract_warcs(warc_files):\n",
    "    \"\"\"Extraction function using FastWARC, yielding records for creating a Spark RDD, based on Save Page Now methodology\"\"\"\n",
    "    for warc_file in warc_files:\n",
    "        with open(warc_file, 'rb') as stream:\n",
    "            for record in ArchiveIterator(stream, func_filter=lambda x: x.headers.get('WARC-Type')=='response'):\n",
    "                yield tuple([record.headers.get(i) for i in sorted_header_keys]+[record.http_headers.get(i) for i in sorted_http_header_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74713"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warcs = sc.parallelize(WARCS)\n",
    "results = warcs.mapPartitions(extract_warcs)\n",
    "results_df = spark.createDataFrame(results, schema)\n",
    "results_df.cache()\n",
    "results_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+--------------------+\n",
      "|content_length|        content_type|           warc_date|      warc_record_id|content_language|       last_modified|referer|         server_name|\n",
      "+--------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+--------------------+\n",
      "|         30124|application/http;...|2023-02-06T05:59:35Z|<urn:uuid:c1162ac...|            NULL|                NULL|   NULL|               nginx|\n",
      "|        309861|application/http;...|2023-02-06T07:20:15Z|<urn:uuid:baff24c...|            NULL|                NULL|   NULL|               nginx|\n",
      "|         38628|application/http;...|2023-02-06T06:14:31Z|<urn:uuid:768ec28...|            NULL|                NULL|   NULL|              Apache|\n",
      "|           336|application/http;...|2023-02-06T05:53:28Z|<urn:uuid:6cb60ab...|            NULL|Tue, 08 Sep 2020 ...|   NULL|        nginx/1.16.1|\n",
      "|         43102|application/http;...|2023-02-06T05:27:42Z|<urn:uuid:5fc3209...|            NULL|Mon, 06 Feb 2023 ...|   NULL|nginx/1.18.0 (Ubu...|\n",
      "+--------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View a few rows (suppressing uri to avoid hyperlink generation)\n",
    "results_df.drop('warc_target_uri').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually pull this result into a Pandas dataframe to get a quick assessment of any potential sparsity in the data. As we can see, general metadata elements pertaining to the WARC are well populated, though elements in the HTTP headers are not, and may require ingestion of many more files to glean insights. We also have the option of ingesting the raw HTML as well, and we may wish to process these larger text chunks in a Spark environment as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_length</th>\n",
       "      <th>content_type</th>\n",
       "      <th>warc_date</th>\n",
       "      <th>warc_record_id</th>\n",
       "      <th>warc_target_uri</th>\n",
       "      <th>content_language</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>referer</th>\n",
       "      <th>server_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74713</td>\n",
       "      <td>74713</td>\n",
       "      <td>74713</td>\n",
       "      <td>74713</td>\n",
       "      <td>74713</td>\n",
       "      <td>8720</td>\n",
       "      <td>14808</td>\n",
       "      <td>6</td>\n",
       "      <td>70115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>63376</td>\n",
       "      <td>1</td>\n",
       "      <td>18069</td>\n",
       "      <td>74713</td>\n",
       "      <td>74713</td>\n",
       "      <td>254</td>\n",
       "      <td>13485</td>\n",
       "      <td>4</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1754</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>2023-09-24T20:33:42Z</td>\n",
       "      <td>&lt;urn:uuid:c1162acc-54e4-4178-9009-dec83781ddbe&gt;</td>\n",
       "      <td>http://0337.com.cn/news/shownews.php?id=937</td>\n",
       "      <td>en</td>\n",
       "      <td>Thu, 01 Jan 1970 00:00:00 GMT</td>\n",
       "      <td></td>\n",
       "      <td>nginx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>74713</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4193</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>16016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       content_length                        content_type  \\\n",
       "count           74713                               74713   \n",
       "unique          63376                                   1   \n",
       "top              1754  application/http; msgtype=response   \n",
       "freq               16                               74713   \n",
       "\n",
       "                   warc_date                                   warc_record_id  \\\n",
       "count                  74713                                            74713   \n",
       "unique                 18069                                            74713   \n",
       "top     2023-09-24T20:33:42Z  <urn:uuid:c1162acc-54e4-4178-9009-dec83781ddbe>   \n",
       "freq                      14                                                1   \n",
       "\n",
       "                                    warc_target_uri content_language  \\\n",
       "count                                         74713             8720   \n",
       "unique                                        74713              254   \n",
       "top     http://0337.com.cn/news/shownews.php?id=937               en   \n",
       "freq                                              1             4193   \n",
       "\n",
       "                        last_modified referer server_name  \n",
       "count                           14808       6       70115  \n",
       "unique                          13485       4        1964  \n",
       "top     Thu, 01 Jan 1970 00:00:00 GMT               nginx  \n",
       "freq                               16       3       16016  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic assessment of selected features, ingested as strings\n",
    "results_df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also potentially leverage Spark for examining raw html at scale, though we may still need to be cognizant of memory limits in the driver. When working on a local machine, attempting to ingest all records will lead to memory errors. For the purposes of this EDA (though we might want to employ a similar method later), we can test ingesting only the first 200 characters. From here, we can start to explore this text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Generator to extract just the first 200 characters from the HTML payload\n",
    "def extract_warc_html(warc_files):\n",
    "    \"\"\"Extraction function using FastWARC, yielding records for creating a Spark RDD, based on Save Page Now methodology\"\"\"\n",
    "    for warc_file in warc_files:\n",
    "        with open(warc_file, 'rb') as stream:\n",
    "            for record in ArchiveIterator(stream, func_filter=lambda x: x.headers.get('WARC-Type')=='response'):\n",
    "                yield (record.reader.read().decode('utf-8', errors='replace')[0:200],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map this function as before\n",
    "warcs = sc.parallelize(WARCS)\n",
    "html_data = warcs.mapPartitions(extract_warc_html)\n",
    "html_data_df = html_data.toDF().withColumnRenamed('_1', 'raw_html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can use regular expressions to begin searching for interesting data elements like page titles. This could be performed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   title|\n",
      "+--------+\n",
      "|Infiniti|\n",
      "|Findings|\n",
      "|    News|\n",
      "| Scholia|\n",
      "| Welcome|\n",
      "|   About|\n",
      "| Donetsk|\n",
      "|Pesumati|\n",
      "|    News|\n",
      "|Promocje|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_data_df.select(F.regexp_extract('raw_html', r\"<title>([A-Za-z0-9]*?)</title>\", 1).alias('title')) \\\n",
    "    .filter(F.col('title')!='') \\\n",
    "    .sample(fraction=0.05) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "In the next steps of this project, we will begin prototyping the ingestion pipeline in earnest, combining the downloading process and expanding on the Pyspark ingestion processes performed here as part of EDA. We will also perform data cleansing and additional transformations steps at this point to begin shaping the data into its final format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
